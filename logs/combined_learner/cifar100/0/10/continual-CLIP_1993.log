2026-01-05 21:17:32,595 [trainer.py] => config: exps/10,10_cifar100.json
2026-01-05 21:17:32,595 [trainer.py] => dataset: cifar100
2026-01-05 21:17:32,595 [trainer.py] => shuffle: True
2026-01-05 21:17:32,595 [trainer.py] => init_cls: 10
2026-01-05 21:17:32,595 [trainer.py] => increment: 10
2026-01-05 21:17:32,595 [trainer.py] => device: [device(type='cuda', index=0)]
2026-01-05 21:17:32,596 [trainer.py] => sim: 1
2026-01-05 21:17:32,596 [trainer.py] => epochs: 31
2026-01-05 21:17:32,596 [trainer.py] => lr: 0.01
2026-01-05 21:17:32,596 [trainer.py] => FB_epoch: 51
2026-01-05 21:17:32,596 [trainer.py] => FB_epoch_inc: 31
2026-01-05 21:17:32,596 [trainer.py] => FB_lr_init: 0.0008
2026-01-05 21:17:32,596 [trainer.py] => FB_lr_inc: 0.0008
2026-01-05 21:17:32,596 [trainer.py] => pool: 10
2026-01-05 21:17:32,596 [trainer.py] => sg_num: 1
2026-01-05 21:17:32,596 [trainer.py] => gamma: 0.1
2026-01-05 21:17:32,596 [trainer.py] => milestones: [30, 40, 50]
2026-01-05 21:17:32,597 [trainer.py] => prefix: continual-CLIP
2026-01-05 21:17:32,597 [trainer.py] => model_name: combined_learner
2026-01-05 21:17:32,597 [trainer.py] => model_size: ViT-B/16
2026-01-05 21:17:32,597 [trainer.py] => convnet_type: none
2026-01-05 21:17:32,597 [trainer.py] => seed: 1993
2026-01-05 21:17:32,597 [trainer.py] => batch_size: 64
2026-01-05 21:17:32,597 [trainer.py] => num_workers: 8
2026-01-05 21:17:32,597 [trainer.py] => model_type: clip
2026-01-05 21:17:32,597 [trainer.py] => division_power: 5
2026-01-05 21:17:32,597 [trainer.py] => linear_model: ['linear', 'bn', 'linear']
2026-01-05 21:17:32,597 [trainer.py] => score_model: ['bn', 'linear']
2026-01-05 21:17:32,598 [trainer.py] => regularizer: mahalanobis
2026-01-05 21:17:32,598 [trainer.py] => print_freq: 10
2026-01-05 21:17:32,598 [trainer.py] => memory_size: 0
2026-01-05 21:17:32,598 [trainer.py] => k: 10
2026-01-05 21:17:32,598 [trainer.py] => sim_coeff: 1.0
2026-01-05 21:17:32,598 [trainer.py] => sparse_coeff: 0.001
2026-01-05 21:17:32,598 [trainer.py] => repeat: False
2026-01-05 21:17:32,598 [trainer.py] => interpolation_ratio: 0.3
2026-01-05 21:17:32,598 [trainer.py] => num_archetypes: 4
2026-01-05 21:17:32,598 [trainer.py] => augment: 0
2026-01-05 21:17:53,390 [trainer.py] => config: exps/10,10_cifar100.json
2026-01-05 21:17:53,391 [trainer.py] => dataset: cifar100
2026-01-05 21:17:53,391 [trainer.py] => shuffle: True
2026-01-05 21:17:53,391 [trainer.py] => init_cls: 10
2026-01-05 21:17:53,391 [trainer.py] => increment: 10
2026-01-05 21:17:53,391 [trainer.py] => device: [device(type='cuda', index=0)]
2026-01-05 21:17:53,391 [trainer.py] => sim: 1
2026-01-05 21:17:53,393 [trainer.py] => epochs: 31
2026-01-05 21:17:53,394 [trainer.py] => lr: 0.01
2026-01-05 21:17:53,394 [trainer.py] => FB_epoch: 51
2026-01-05 21:17:53,394 [trainer.py] => FB_epoch_inc: 31
2026-01-05 21:17:53,394 [trainer.py] => FB_lr_init: 0.0008
2026-01-05 21:17:53,394 [trainer.py] => FB_lr_inc: 0.0008
2026-01-05 21:17:53,395 [trainer.py] => pool: 10
2026-01-05 21:17:53,395 [trainer.py] => sg_num: 1
2026-01-05 21:17:53,395 [trainer.py] => gamma: 0.1
2026-01-05 21:17:53,395 [trainer.py] => milestones: [30, 40, 50]
2026-01-05 21:17:53,395 [trainer.py] => prefix: continual-CLIP
2026-01-05 21:17:53,395 [trainer.py] => model_name: combined_learner
2026-01-05 21:17:53,395 [trainer.py] => model_size: ViT-B/16
2026-01-05 21:17:53,395 [trainer.py] => convnet_type: none
2026-01-05 21:17:53,395 [trainer.py] => seed: 1993
2026-01-05 21:17:53,396 [trainer.py] => batch_size: 64
2026-01-05 21:17:53,396 [trainer.py] => num_workers: 8
2026-01-05 21:17:53,396 [trainer.py] => model_type: clip
2026-01-05 21:17:53,396 [trainer.py] => division_power: 5
2026-01-05 21:17:53,396 [trainer.py] => linear_model: ['linear', 'bn', 'linear']
2026-01-05 21:17:53,396 [trainer.py] => score_model: ['bn', 'linear']
2026-01-05 21:17:53,396 [trainer.py] => regularizer: mahalanobis
2026-01-05 21:17:53,396 [trainer.py] => print_freq: 10
2026-01-05 21:17:53,396 [trainer.py] => memory_size: 0
2026-01-05 21:17:53,396 [trainer.py] => k: 10
2026-01-05 21:17:53,397 [trainer.py] => sim_coeff: 1.0
2026-01-05 21:17:53,397 [trainer.py] => sparse_coeff: 0.001
2026-01-05 21:17:53,397 [trainer.py] => repeat: False
2026-01-05 21:17:53,397 [trainer.py] => interpolation_ratio: 0.3
2026-01-05 21:17:53,397 [trainer.py] => num_archetypes: 4
2026-01-05 21:17:53,397 [trainer.py] => augment: 0
2026-01-05 21:18:42,941 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2026-01-05 21:18:43,095 [SGC.py] => Learning on task 0 (10 new classes): 0-9
2026-01-05 21:18:43,106 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2026-01-05 21:18:43,121 [SGC.py] => Building prototypes and covariance for new classes...
2026-01-05 21:18:43,293 [SGC.py] => Learning on task 0 (10 new classes): 0-9
2026-01-05 21:18:43,313 [SGC.py] => Building prototypes and covariance for new classes...
2026-01-05 21:19:49,464 [SGC.py] => Part 1: Grouping new classes...
2026-01-05 21:19:49,698 [SGC.py] => Part 1: Grouping new classes...
2026-01-05 21:20:18,926 [SGC.py] => Task has 10 classes. Using standard grouping strategy.
2026-01-05 21:20:18,926 [SGC.py] => Task size is even. Splitting into two groups via Max-Cut.
2026-01-05 21:20:18,930 [SGC.py] => Grouping complete. Current p2c map: {1: [0, 2, 5, 7, 9], 2: [1, 3, 4, 6, 8]}
2026-01-05 21:20:18,932 [SGC.py] => Building feature set using Balanced Strategy (Interpolation + Gaussian)...
2026-01-05 21:20:19,246 [SGC.py] => Task has 10 classes. Using standard grouping strategy.
2026-01-05 21:20:19,246 [SGC.py] => Task size is even. Splitting into two groups via Max-Cut.
2026-01-05 21:20:19,247 [SGC.py] => Grouping complete. Current p2c map: {1: [0, 2, 5, 7, 9], 2: [1, 3, 4, 6, 8]}
2026-01-05 21:20:19,251 [SGC.py] => Building feature set using Balanced Strategy (Interpolation + Gaussian)...
2026-01-05 21:20:48,415 [SGC.py] => Starting model training. Training ALL 2 groups.
2026-01-05 21:20:48,416 [SGC.py] => Unfreezing components for ALL groups: [1, 2]
2026-01-05 21:20:48,953 [SGC.py] => Starting model training. Training ALL 2 groups.
2026-01-05 21:20:48,955 [SGC.py] => Unfreezing components for ALL groups: [1, 2]
2026-01-05 21:21:27,854 [SGC.py] => Task: 0, Epoch: 1, Train Loss: 2.076534, Train Acc: 73.7200, Test Acc on New: 93.7000
2026-01-05 21:21:27,886 [SGC.py] => New best accuracy found: 93.70%
2026-01-05 21:21:28,100 [SGC.py] => Task: 0, Epoch: 1, Train Loss: 2.076534, Train Acc: 73.7200, Test Acc on New: 93.7000
2026-01-05 21:21:28,130 [SGC.py] => New best accuracy found: 93.70%
2026-01-05 21:23:29,509 [SGC.py] => Task: 0, Epoch: 11, Train Loss: 0.101127, Train Acc: 97.8200, Test Acc on New: 96.7000
2026-01-05 21:23:29,541 [SGC.py] => New best accuracy found: 96.70%
2026-01-05 21:23:46,055 [SGC.py] => Task: 0, Epoch: 11, Train Loss: 0.101127, Train Acc: 97.8200, Test Acc on New: 96.7000
2026-01-05 21:23:46,078 [SGC.py] => New best accuracy found: 96.70%
2026-01-05 21:25:21,999 [SGC.py] => Task: 0, Epoch: 21, Train Loss: 0.056427, Train Acc: 98.7800, Test Acc on New: 97.4000
2026-01-05 21:25:22,016 [SGC.py] => New best accuracy found: 97.40%
2026-01-05 21:28:17,056 [SGC.py] => Task: 0, Epoch: 31, Train Loss: 0.037501, Train Acc: 99.4400, Test Acc on New: 97.6000
2026-01-05 21:28:17,081 [SGC.py] => New best accuracy found: 97.60%
2026-01-05 21:30:10,114 [SGC.py] => Task: 0, Epoch: 41, Train Loss: 0.036076, Train Acc: 99.4800, Test Acc on New: 97.3000
2026-01-05 21:33:32,482 [SGC.py] => Task: 0, Epoch: 51, Train Loss: 0.037135, Train Acc: 99.5000, Test Acc on New: 97.3000
2026-01-05 21:33:32,482 [SGC.py] => Loading best model for task 0 with Test Acc on New: 97.60%
2026-01-05 21:34:09,738 [toolkit.py] => Calculating accuracy per defined class group.
2026-01-05 21:34:09,739 [SGC.py] => CNN accuracy: {'total': np.float64(97.6), 'group_1': np.float64(97.6), 'group_2': np.float64(97.6), 'old': 0, 'new': np.float64(97.6), 'top1': np.float64(97.6), 'top5': np.float64(100.0), 'grouped': {'total': np.float64(97.6), 'group_1': np.float64(97.6), 'group_2': np.float64(97.6), 'old': 0, 'new': np.float64(97.6), 'top1': np.float64(97.6), 'top5': np.float64(100.0)}}
2026-01-05 21:34:09,739 [trainer.py] => No NME accuracy.
2026-01-05 21:34:09,739 [trainer.py] => CNN: {'total': np.float64(97.6), 'group_1': np.float64(97.6), 'group_2': np.float64(97.6), 'old': 0, 'new': np.float64(97.6), 'top1': np.float64(97.6), 'top5': np.float64(100.0)}
2026-01-05 21:34:09,740 [trainer.py] => CNN top1 curve: [np.float64(97.6)]
2026-01-05 21:34:09,740 [trainer.py] => CNN top5 curve: [np.float64(100.0)]

2026-01-05 21:34:09,740 [trainer.py] => Average Accuracy (CNN): 97.6
2026-01-05 21:34:09,740 [SGC.py] => Learning on task 1 (10 new classes): 10-19
2026-01-05 21:34:09,767 [SGC.py] => Building prototypes and covariance for new classes...
2026-01-05 21:35:05,887 [SGC.py] => Part 1: Grouping new classes...
2026-01-05 21:35:38,562 [SGC.py] => Task has 10 classes. Using standard grouping strategy.
2026-01-05 21:35:38,562 [SGC.py] => Task size is even. Splitting into two groups via Max-Cut.
2026-01-05 21:35:38,565 [SGC.py] => Grouping complete. Current p2c map: {1: [0, 2, 5, 7, 9], 2: [1, 3, 4, 6, 8], 3: [10, 11, 16, 17, 18], 4: [12, 13, 14, 15, 19]}
2026-01-05 21:35:38,576 [SGC.py] => Building feature set using Balanced Strategy (Interpolation + Gaussian)...
2026-01-05 21:36:12,042 [SGC.py] => Generating pseudo-features for old classes via mixing...
2026-01-05 21:36:12,197 [SGC.py] => Starting model training. Training ALL 4 groups.
2026-01-05 21:36:12,199 [SGC.py] => Unfreezing components for ALL groups: [1, 2, 3, 4]
2026-01-05 21:37:16,844 [SGC.py] => Task: 1, Epoch: 1, Train Loss: 1.417952, Train Acc: 68.2567, Test Acc on New: 69.3000
2026-01-05 21:37:16,896 [SGC.py] => New best accuracy found: 69.30%
2026-01-05 21:41:15,708 [SGC.py] => Task: 1, Epoch: 11, Train Loss: 0.166792, Train Acc: 95.4916, Test Acc on New: 91.7000
2026-01-05 21:41:15,759 [SGC.py] => New best accuracy found: 91.70%
2026-01-05 21:45:00,970 [SGC.py] => Task: 1, Epoch: 21, Train Loss: 0.120992, Train Acc: 96.9168, Test Acc on New: 92.0000
2026-01-05 21:45:01,023 [SGC.py] => New best accuracy found: 92.00%
2026-01-05 21:48:57,185 [SGC.py] => Task: 1, Epoch: 31, Train Loss: 0.092166, Train Acc: 97.9445, Test Acc on New: 93.0000
2026-01-05 21:48:57,204 [SGC.py] => New best accuracy found: 93.00%
2026-01-05 21:48:57,205 [SGC.py] => Loading best model for task 1 with Test Acc on New: 93.00%
2026-01-05 21:49:28,004 [toolkit.py] => Calculating accuracy per defined class group.
2026-01-05 21:49:28,005 [SGC.py] => CNN accuracy: {'total': np.float64(93.5), 'group_1': np.float64(93.6), 'group_2': np.float64(94.4), 'group_3': np.float64(94.4), 'group_4': np.float64(91.6), 'old': np.float64(94.0), 'new': np.float64(93.0), 'top1': np.float64(93.5), 'top5': np.float64(99.2), 'grouped': {'total': np.float64(93.5), 'group_1': np.float64(93.6), 'group_2': np.float64(94.4), 'group_3': np.float64(94.4), 'group_4': np.float64(91.6), 'old': np.float64(94.0), 'new': np.float64(93.0), 'top1': np.float64(93.5), 'top5': np.float64(99.2)}}
2026-01-05 21:49:28,005 [trainer.py] => No NME accuracy.
2026-01-05 21:49:28,005 [trainer.py] => CNN: {'total': np.float64(93.5), 'group_1': np.float64(93.6), 'group_2': np.float64(94.4), 'group_3': np.float64(94.4), 'group_4': np.float64(91.6), 'old': np.float64(94.0), 'new': np.float64(93.0), 'top1': np.float64(93.5), 'top5': np.float64(99.2)}
2026-01-05 21:49:28,005 [trainer.py] => CNN top1 curve: [np.float64(97.6), np.float64(93.5)]
2026-01-05 21:49:28,006 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(99.2)]

2026-01-05 21:49:28,006 [trainer.py] => Average Accuracy (CNN): 95.55
2026-01-05 21:49:28,006 [SGC.py] => Learning on task 2 (10 new classes): 20-29
2026-01-05 21:49:28,029 [SGC.py] => Building prototypes and covariance for new classes...
2026-01-05 21:50:04,449 [SGC.py] => Part 1: Grouping new classes...
2026-01-05 21:50:27,785 [SGC.py] => Task has 10 classes. Using standard grouping strategy.
2026-01-05 21:50:27,785 [SGC.py] => Task size is even. Splitting into two groups via Max-Cut.
2026-01-05 21:50:27,786 [SGC.py] => Grouping complete. Current p2c map: {1: [0, 2, 5, 7, 9], 2: [1, 3, 4, 6, 8], 3: [10, 11, 16, 17, 18], 4: [12, 13, 14, 15, 19], 5: [20, 22, 26, 27, 29], 6: [21, 23, 24, 25, 28]}
2026-01-05 21:50:27,791 [SGC.py] => Building feature set using Balanced Strategy (Interpolation + Gaussian)...
2026-01-05 21:50:52,408 [SGC.py] => Generating pseudo-features for old classes via mixing...
2026-01-05 21:50:52,473 [SGC.py] => Starting model training. Training ALL 6 groups.
2026-01-05 21:50:52,475 [SGC.py] => Unfreezing components for ALL groups: [1, 2, 3, 4, 5, 6]
2026-01-05 21:51:55,841 [SGC.py] => Task: 2, Epoch: 1, Train Loss: 1.141084, Train Acc: 77.4273, Test Acc on New: 75.3000
2026-01-05 21:51:55,886 [SGC.py] => New best accuracy found: 75.30%
2026-01-05 21:55:48,333 [SGC.py] => Task: 2, Epoch: 11, Train Loss: 0.143771, Train Acc: 96.2573, Test Acc on New: 92.1000
2026-01-05 21:55:48,350 [SGC.py] => New best accuracy found: 92.10%
2026-01-05 22:00:07,729 [SGC.py] => Task: 2, Epoch: 21, Train Loss: 0.104071, Train Acc: 97.6988, Test Acc on New: 94.1000
2026-01-05 22:00:07,744 [SGC.py] => New best accuracy found: 94.10%
2026-01-05 22:04:18,643 [SGC.py] => Task: 2, Epoch: 31, Train Loss: 0.076357, Train Acc: 98.7201, Test Acc on New: 94.0000
2026-01-05 22:04:18,643 [SGC.py] => Loading best model for task 2 with Test Acc on New: 94.10%
2026-01-05 22:05:09,204 [toolkit.py] => Calculating accuracy per defined class group.
2026-01-05 22:05:09,205 [SGC.py] => CNN accuracy: {'total': np.float64(91.13), 'group_1': np.float64(93.4), 'group_2': np.float64(89.2), 'group_3': np.float64(89.2), 'group_4': np.float64(86.8), 'group_5': np.float64(94.2), 'group_6': np.float64(94.0), 'old': np.float64(89.65), 'new': np.float64(94.1), 'top1': np.float64(91.13), 'top5': np.float64(98.83), 'grouped': {'total': np.float64(91.13), 'group_1': np.float64(93.4), 'group_2': np.float64(89.2), 'group_3': np.float64(89.2), 'group_4': np.float64(86.8), 'group_5': np.float64(94.2), 'group_6': np.float64(94.0), 'old': np.float64(89.65), 'new': np.float64(94.1), 'top1': np.float64(91.13), 'top5': np.float64(98.83)}}
2026-01-05 22:05:09,205 [trainer.py] => No NME accuracy.
2026-01-05 22:05:09,206 [trainer.py] => CNN: {'total': np.float64(91.13), 'group_1': np.float64(93.4), 'group_2': np.float64(89.2), 'group_3': np.float64(89.2), 'group_4': np.float64(86.8), 'group_5': np.float64(94.2), 'group_6': np.float64(94.0), 'old': np.float64(89.65), 'new': np.float64(94.1), 'top1': np.float64(91.13), 'top5': np.float64(98.83)}
2026-01-05 22:05:09,206 [trainer.py] => CNN top1 curve: [np.float64(97.6), np.float64(93.5), np.float64(91.13)]
2026-01-05 22:05:09,206 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(99.2), np.float64(98.83)]

2026-01-05 22:05:09,208 [trainer.py] => Average Accuracy (CNN): 94.07666666666667
2026-01-05 22:05:09,210 [SGC.py] => Learning on task 3 (10 new classes): 30-39
2026-01-05 22:05:09,299 [SGC.py] => Building prototypes and covariance for new classes...
2026-01-05 22:06:04,812 [SGC.py] => Part 1: Grouping new classes...
2026-01-05 22:06:36,962 [SGC.py] => Task has 10 classes. Using standard grouping strategy.
2026-01-05 22:06:36,962 [SGC.py] => Task size is even. Splitting into two groups via Max-Cut.
2026-01-05 22:06:36,965 [SGC.py] => Grouping complete. Current p2c map: {1: [0, 2, 5, 7, 9], 2: [1, 3, 4, 6, 8], 3: [10, 11, 16, 17, 18], 4: [12, 13, 14, 15, 19], 5: [20, 22, 26, 27, 29], 6: [21, 23, 24, 25, 28], 7: [30, 34, 35, 36, 39], 8: [31, 32, 33, 37, 38]}
2026-01-05 22:06:36,973 [SGC.py] => Building feature set using Balanced Strategy (Interpolation + Gaussian)...
2026-01-05 22:07:09,821 [SGC.py] => Generating pseudo-features for old classes via mixing...
2026-01-05 22:07:09,935 [SGC.py] => Starting model training. Training ALL 8 groups.
2026-01-05 22:07:09,937 [SGC.py] => Unfreezing components for ALL groups: [1, 2, 3, 4, 5, 6, 7, 8]
2026-01-05 22:08:25,461 [SGC.py] => Task: 3, Epoch: 1, Train Loss: 0.937200, Train Acc: 80.3840, Test Acc on New: 68.7000
2026-01-05 22:08:25,495 [SGC.py] => New best accuracy found: 68.70%
2026-01-05 22:14:04,858 [SGC.py] => Task: 3, Epoch: 11, Train Loss: 0.176229, Train Acc: 95.3602, Test Acc on New: 89.0000
2026-01-05 22:14:04,894 [SGC.py] => New best accuracy found: 89.00%
2026-01-05 22:19:35,843 [SGC.py] => Task: 3, Epoch: 21, Train Loss: 0.127746, Train Acc: 97.1735, Test Acc on New: 89.6000
2026-01-05 22:19:35,896 [SGC.py] => New best accuracy found: 89.60%
2026-01-05 22:25:14,626 [SGC.py] => Task: 3, Epoch: 31, Train Loss: 0.096063, Train Acc: 98.3661, Test Acc on New: 89.7000
2026-01-05 22:25:14,684 [SGC.py] => New best accuracy found: 89.70%
2026-01-05 22:25:14,685 [SGC.py] => Loading best model for task 3 with Test Acc on New: 89.70%
2026-01-05 22:26:06,520 [toolkit.py] => Calculating accuracy per defined class group.
2026-01-05 22:26:06,521 [SGC.py] => CNN accuracy: {'total': np.float64(88.9), 'group_1': np.float64(91.4), 'group_2': np.float64(86.6), 'group_3': np.float64(86.2), 'group_4': np.float64(87.0), 'group_5': np.float64(90.2), 'group_6': np.float64(90.4), 'group_7': np.float64(89.6), 'group_8': np.float64(89.8), 'old': np.float64(88.63), 'new': np.float64(89.7), 'top1': np.float64(88.9), 'top5': np.float64(98.62), 'grouped': {'total': np.float64(88.9), 'group_1': np.float64(91.4), 'group_2': np.float64(86.6), 'group_3': np.float64(86.2), 'group_4': np.float64(87.0), 'group_5': np.float64(90.2), 'group_6': np.float64(90.4), 'group_7': np.float64(89.6), 'group_8': np.float64(89.8), 'old': np.float64(88.63), 'new': np.float64(89.7), 'top1': np.float64(88.9), 'top5': np.float64(98.62)}}
2026-01-05 22:26:06,521 [trainer.py] => No NME accuracy.
2026-01-05 22:26:06,522 [trainer.py] => CNN: {'total': np.float64(88.9), 'group_1': np.float64(91.4), 'group_2': np.float64(86.6), 'group_3': np.float64(86.2), 'group_4': np.float64(87.0), 'group_5': np.float64(90.2), 'group_6': np.float64(90.4), 'group_7': np.float64(89.6), 'group_8': np.float64(89.8), 'old': np.float64(88.63), 'new': np.float64(89.7), 'top1': np.float64(88.9), 'top5': np.float64(98.62)}
2026-01-05 22:26:06,522 [trainer.py] => CNN top1 curve: [np.float64(97.6), np.float64(93.5), np.float64(91.13), np.float64(88.9)]
2026-01-05 22:26:06,522 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(99.2), np.float64(98.83), np.float64(98.62)]

2026-01-05 22:26:06,522 [trainer.py] => Average Accuracy (CNN): 92.7825
2026-01-05 22:26:06,522 [SGC.py] => Learning on task 4 (10 new classes): 40-49
2026-01-05 22:26:06,567 [SGC.py] => Building prototypes and covariance for new classes...
2026-01-05 22:27:02,127 [SGC.py] => Part 1: Grouping new classes...
2026-01-05 22:27:34,182 [SGC.py] => Task has 10 classes. Using standard grouping strategy.
2026-01-05 22:27:34,182 [SGC.py] => Task size is even. Splitting into two groups via Max-Cut.
2026-01-05 22:27:34,185 [SGC.py] => Grouping complete. Current p2c map: {1: [0, 2, 5, 7, 9], 2: [1, 3, 4, 6, 8], 3: [10, 11, 16, 17, 18], 4: [12, 13, 14, 15, 19], 5: [20, 22, 26, 27, 29], 6: [21, 23, 24, 25, 28], 7: [30, 34, 35, 36, 39], 8: [31, 32, 33, 37, 38], 9: [40, 43, 46, 48, 49], 10: [41, 42, 44, 45, 47]}
2026-01-05 22:27:34,204 [SGC.py] => Building feature set using Balanced Strategy (Interpolation + Gaussian)...
2026-01-05 22:28:06,349 [SGC.py] => Generating pseudo-features for old classes via mixing...
2026-01-05 22:28:06,471 [SGC.py] => Starting model training. Training ALL 10 groups.
2026-01-05 22:28:06,472 [SGC.py] => Unfreezing components for ALL groups: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2026-01-05 22:29:27,157 [SGC.py] => Task: 4, Epoch: 1, Train Loss: 0.890314, Train Acc: 81.6482, Test Acc on New: 70.4000
2026-01-05 22:29:27,207 [SGC.py] => New best accuracy found: 70.40%
2026-01-05 22:36:14,623 [SGC.py] => Task: 4, Epoch: 11, Train Loss: 0.182889, Train Acc: 95.4315, Test Acc on New: 89.5000
2026-01-05 22:36:14,689 [SGC.py] => New best accuracy found: 89.50%
2026-01-05 22:43:08,318 [SGC.py] => Task: 4, Epoch: 21, Train Loss: 0.135165, Train Acc: 97.1922, Test Acc on New: 90.1000
2026-01-05 22:43:08,374 [SGC.py] => New best accuracy found: 90.10%
2026-01-05 22:50:00,988 [SGC.py] => Task: 4, Epoch: 31, Train Loss: 0.101199, Train Acc: 98.4642, Test Acc on New: 89.7000
2026-01-05 22:50:00,988 [SGC.py] => Loading best model for task 4 with Test Acc on New: 90.10%
2026-01-05 22:50:55,097 [toolkit.py] => Calculating accuracy per defined class group.
2026-01-05 22:50:55,099 [SGC.py] => CNN accuracy: {'total': np.float64(86.42), 'group_1': np.float64(89.2), 'group_2': np.float64(85.6), 'group_3': np.float64(87.6), 'group_4': np.float64(78.6), 'group_5': np.float64(88.0), 'group_6': np.float64(87.8), 'group_7': np.float64(84.4), 'group_8': np.float64(82.8), 'group_9': np.float64(92.6), 'group_10': np.float64(87.6), 'old': np.float64(85.5), 'new': np.float64(90.1), 'top1': np.float64(86.42), 'top5': np.float64(98.3), 'grouped': {'total': np.float64(86.42), 'group_1': np.float64(89.2), 'group_2': np.float64(85.6), 'group_3': np.float64(87.6), 'group_4': np.float64(78.6), 'group_5': np.float64(88.0), 'group_6': np.float64(87.8), 'group_7': np.float64(84.4), 'group_8': np.float64(82.8), 'group_9': np.float64(92.6), 'group_10': np.float64(87.6), 'old': np.float64(85.5), 'new': np.float64(90.1), 'top1': np.float64(86.42), 'top5': np.float64(98.3)}}
2026-01-05 22:50:55,099 [trainer.py] => No NME accuracy.
2026-01-05 22:50:55,099 [trainer.py] => CNN: {'total': np.float64(86.42), 'group_1': np.float64(89.2), 'group_2': np.float64(85.6), 'group_3': np.float64(87.6), 'group_4': np.float64(78.6), 'group_5': np.float64(88.0), 'group_6': np.float64(87.8), 'group_7': np.float64(84.4), 'group_8': np.float64(82.8), 'group_9': np.float64(92.6), 'group_10': np.float64(87.6), 'old': np.float64(85.5), 'new': np.float64(90.1), 'top1': np.float64(86.42), 'top5': np.float64(98.3)}
2026-01-05 22:50:55,099 [trainer.py] => CNN top1 curve: [np.float64(97.6), np.float64(93.5), np.float64(91.13), np.float64(88.9), np.float64(86.42)]
2026-01-05 22:50:55,099 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(99.2), np.float64(98.83), np.float64(98.62), np.float64(98.3)]

2026-01-05 22:50:55,100 [trainer.py] => Average Accuracy (CNN): 91.51
2026-01-05 22:50:55,101 [SGC.py] => Learning on task 5 (10 new classes): 50-59
2026-01-05 22:50:55,151 [SGC.py] => Building prototypes and covariance for new classes...
2026-01-05 22:51:51,133 [SGC.py] => Part 1: Grouping new classes...
2026-01-05 22:52:23,361 [SGC.py] => Task has 10 classes. Using standard grouping strategy.
2026-01-05 22:52:23,361 [SGC.py] => Task size is even. Splitting into two groups via Max-Cut.
2026-01-05 22:52:23,364 [SGC.py] => Grouping complete. Current p2c map: {1: [0, 2, 5, 7, 9], 2: [1, 3, 4, 6, 8], 3: [10, 11, 16, 17, 18], 4: [12, 13, 14, 15, 19], 5: [20, 22, 26, 27, 29], 6: [21, 23, 24, 25, 28], 7: [30, 34, 35, 36, 39], 8: [31, 32, 33, 37, 38], 9: [40, 43, 46, 48, 49], 10: [41, 42, 44, 45, 47], 11: [50, 51, 52, 56, 57], 12: [53, 54, 55, 58, 59]}
2026-01-05 22:52:23,385 [SGC.py] => Building feature set using Balanced Strategy (Interpolation + Gaussian)...
2026-01-05 22:52:56,571 [SGC.py] => Generating pseudo-features for old classes via mixing...
2026-01-05 22:52:56,799 [SGC.py] => Starting model training. Training ALL 12 groups.
2026-01-05 22:52:56,801 [SGC.py] => Unfreezing components for ALL groups: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
2026-01-05 22:54:28,251 [SGC.py] => Task: 5, Epoch: 1, Train Loss: 0.696778, Train Acc: 84.9202, Test Acc on New: 71.7000
2026-01-05 22:54:28,304 [SGC.py] => New best accuracy found: 71.70%
2026-01-05 23:02:28,225 [SGC.py] => Task: 5, Epoch: 11, Train Loss: 0.190060, Train Acc: 95.3881, Test Acc on New: 88.0000
2026-01-05 23:02:28,278 [SGC.py] => New best accuracy found: 88.00%
2026-01-16 00:30:08,930 [trainer.py] => config: exps/10,10_cifar100.json
2026-01-16 00:30:08,930 [trainer.py] => dataset: cifar100
2026-01-16 00:30:08,930 [trainer.py] => shuffle: True
2026-01-16 00:30:08,930 [trainer.py] => init_cls: 10
2026-01-16 00:30:08,930 [trainer.py] => increment: 10
2026-01-16 00:30:08,930 [trainer.py] => device: [device(type='cuda', index=0)]
2026-01-16 00:30:08,930 [trainer.py] => sim: 1
2026-01-16 00:30:08,930 [trainer.py] => epochs: 31
2026-01-16 00:30:08,930 [trainer.py] => lr: 0.01
2026-01-16 00:30:08,930 [trainer.py] => FB_epoch: 51
2026-01-16 00:30:08,945 [trainer.py] => FB_epoch_inc: 31
2026-01-16 00:30:08,945 [trainer.py] => FB_lr_init: 0.0008
2026-01-16 00:30:08,945 [trainer.py] => FB_lr_inc: 0.0008
2026-01-16 00:30:08,945 [trainer.py] => pool: 10
2026-01-16 00:30:08,945 [trainer.py] => sg_num: 1
2026-01-16 00:30:08,945 [trainer.py] => gamma: 0.1
2026-01-16 00:30:08,945 [trainer.py] => milestones: [30, 40, 50]
2026-01-16 00:30:08,945 [trainer.py] => prefix: continual-CLIP
2026-01-16 00:30:08,945 [trainer.py] => model_name: combined_learner
2026-01-16 00:30:08,945 [trainer.py] => model_size: ViT-B/16
2026-01-16 00:30:08,948 [trainer.py] => convnet_type: none
2026-01-16 00:30:08,948 [trainer.py] => seed: 1993
2026-01-16 00:30:08,948 [trainer.py] => batch_size: 64
2026-01-16 00:30:08,948 [trainer.py] => num_workers: 8
2026-01-16 00:30:08,948 [trainer.py] => model_type: clip
2026-01-16 00:30:08,948 [trainer.py] => division_power: 5
2026-01-16 00:30:08,948 [trainer.py] => linear_model: ['linear', 'bn', 'linear']
2026-01-16 00:30:08,948 [trainer.py] => score_model: ['bn', 'linear']
2026-01-16 00:30:08,948 [trainer.py] => regularizer: mahalanobis
2026-01-16 00:30:08,948 [trainer.py] => print_freq: 10
2026-01-16 00:30:08,948 [trainer.py] => memory_size: 0
2026-01-16 00:30:08,948 [trainer.py] => k: 10
2026-01-16 00:30:08,948 [trainer.py] => sim_coeff: 1.0
2026-01-16 00:30:08,948 [trainer.py] => sparse_coeff: 0.001
2026-01-16 00:30:08,948 [trainer.py] => repeat: False
2026-01-16 00:30:08,948 [trainer.py] => interpolation_ratio: 0.3
2026-01-16 00:30:08,948 [trainer.py] => num_archetypes: 4
2026-01-16 00:30:08,948 [trainer.py] => augment: 0
2026-01-16 00:55:53,287 [trainer.py] => config: exps/10,10_cifar100.json
2026-01-16 00:55:53,287 [trainer.py] => dataset: cifar100
2026-01-16 00:55:53,287 [trainer.py] => shuffle: True
2026-01-16 00:55:53,288 [trainer.py] => init_cls: 10
2026-01-16 00:55:53,288 [trainer.py] => increment: 10
2026-01-16 00:55:53,288 [trainer.py] => device: [device(type='cuda', index=0)]
2026-01-16 00:55:53,291 [trainer.py] => sim: 1
2026-01-16 00:55:53,292 [trainer.py] => epochs: 31
2026-01-16 00:55:53,292 [trainer.py] => lr: 0.01
2026-01-16 00:55:53,292 [trainer.py] => FB_epoch: 51
2026-01-16 00:55:53,292 [trainer.py] => FB_epoch_inc: 31
2026-01-16 00:55:53,293 [trainer.py] => FB_lr_init: 0.0008
2026-01-16 00:55:53,293 [trainer.py] => FB_lr_inc: 0.0008
2026-01-16 00:55:53,293 [trainer.py] => pool: 10
2026-01-16 00:55:53,293 [trainer.py] => sg_num: 1
2026-01-16 00:55:53,293 [trainer.py] => gamma: 0.1
2026-01-16 00:55:53,293 [trainer.py] => milestones: [30, 40, 50]
2026-01-16 00:55:53,294 [trainer.py] => prefix: continual-CLIP
2026-01-16 00:55:53,294 [trainer.py] => model_name: combined_learner
2026-01-16 00:55:53,294 [trainer.py] => model_size: ViT-B/16
2026-01-16 00:55:53,294 [trainer.py] => convnet_type: none
2026-01-16 00:55:53,294 [trainer.py] => seed: 1993
2026-01-16 00:55:53,294 [trainer.py] => batch_size: 64
2026-01-16 00:55:53,294 [trainer.py] => num_workers: 8
2026-01-16 00:55:53,294 [trainer.py] => model_type: clip
2026-01-16 00:55:53,294 [trainer.py] => division_power: 5
2026-01-16 00:55:53,294 [trainer.py] => linear_model: ['linear', 'bn', 'linear']
2026-01-16 00:55:53,294 [trainer.py] => score_model: ['bn', 'linear']
2026-01-16 00:55:53,294 [trainer.py] => regularizer: mahalanobis
2026-01-16 00:55:53,294 [trainer.py] => print_freq: 10
2026-01-16 00:55:53,294 [trainer.py] => memory_size: 0
2026-01-16 00:55:53,294 [trainer.py] => k: 10
2026-01-16 00:55:53,294 [trainer.py] => sim_coeff: 1.0
2026-01-16 00:55:53,294 [trainer.py] => sparse_coeff: 0.001
2026-01-16 00:55:53,294 [trainer.py] => repeat: False
2026-01-16 00:55:53,295 [trainer.py] => interpolation_ratio: 0.3
2026-01-16 00:55:53,295 [trainer.py] => num_archetypes: 4
2026-01-16 00:55:53,295 [trainer.py] => augment: 0
2026-01-16 00:55:56,335 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2026-01-16 00:55:56,425 [SGC.py] => Learning on task 0 (10 new classes): 0-9
2026-01-16 00:55:56,443 [SGC.py] => Building prototypes and covariance for new classes...
2026-01-16 11:41:28,086 [trainer.py] => config: exps/10,10_cifar100.json
2026-01-16 11:41:28,089 [trainer.py] => dataset: cifar100
2026-01-16 11:41:28,090 [trainer.py] => shuffle: True
2026-01-16 11:41:28,090 [trainer.py] => init_cls: 10
2026-01-16 11:41:28,090 [trainer.py] => increment: 10
2026-01-16 11:41:28,090 [trainer.py] => device: [device(type='cuda', index=0)]
2026-01-16 11:41:28,092 [trainer.py] => sim: 1
2026-01-16 11:41:28,092 [trainer.py] => epochs: 31
2026-01-16 11:41:28,093 [trainer.py] => lr: 0.01
2026-01-16 11:41:28,093 [trainer.py] => FB_epoch: 51
2026-01-16 11:41:28,093 [trainer.py] => FB_epoch_inc: 31
2026-01-16 11:41:28,093 [trainer.py] => FB_lr_init: 0.0008
2026-01-16 11:41:28,093 [trainer.py] => FB_lr_inc: 0.0008
2026-01-16 11:41:28,096 [trainer.py] => pool: 10
2026-01-16 11:41:28,096 [trainer.py] => sg_num: 1
2026-01-16 11:41:28,096 [trainer.py] => gamma: 0.1
2026-01-16 11:41:28,096 [trainer.py] => milestones: [30, 40, 50]
2026-01-16 11:41:28,097 [trainer.py] => prefix: continual-CLIP
2026-01-16 11:41:28,097 [trainer.py] => model_name: combined_learner
2026-01-16 11:41:28,097 [trainer.py] => model_size: ViT-B/16
2026-01-16 11:41:28,097 [trainer.py] => convnet_type: none
2026-01-16 11:41:28,097 [trainer.py] => seed: 1993
2026-01-16 11:41:28,097 [trainer.py] => batch_size: 64
2026-01-16 11:41:28,097 [trainer.py] => num_workers: 8
2026-01-16 11:41:28,097 [trainer.py] => model_type: clip
2026-01-16 11:41:28,097 [trainer.py] => division_power: 5
2026-01-16 11:41:28,097 [trainer.py] => linear_model: ['linear', 'bn', 'linear']
2026-01-16 11:41:28,097 [trainer.py] => score_model: ['bn', 'linear']
2026-01-16 11:41:28,097 [trainer.py] => regularizer: mahalanobis
2026-01-16 11:41:28,097 [trainer.py] => print_freq: 10
2026-01-16 11:41:28,098 [trainer.py] => memory_size: 0
2026-01-16 11:41:28,098 [trainer.py] => k: 10
2026-01-16 11:41:28,098 [trainer.py] => sim_coeff: 1.0
2026-01-16 11:41:28,098 [trainer.py] => sparse_coeff: 0.001
2026-01-16 11:41:28,098 [trainer.py] => repeat: False
2026-01-16 11:41:28,098 [trainer.py] => interpolation_ratio: 0.3
2026-01-16 11:41:28,098 [trainer.py] => num_archetypes: 4
2026-01-16 11:41:28,098 [trainer.py] => augment: 0
2026-01-16 11:41:31,618 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2026-01-16 11:41:31,711 [SGC.py] => Learning on task 0 (10 new classes): 0-9
2026-01-16 11:41:31,726 [SGC.py] => Building prototypes and covariance for new classes...
2026-01-16 11:42:13,482 [SGC.py] => Part 1: Grouping new classes...
2026-01-16 11:42:38,213 [SGC.py] => Task has 10 classes. Using standard grouping strategy.
2026-01-16 11:42:38,213 [SGC.py] => Task size is even. Splitting into two groups via Max-Cut.
2026-01-16 11:42:38,216 [SGC.py] => Grouping complete. Current p2c map: {1: [0, 2, 5, 7, 9], 2: [1, 3, 4, 6, 8]}
2026-01-16 11:42:38,221 [SGC.py] => Building feature set using Balanced Strategy (Interpolation + Gaussian)...
2026-01-16 11:43:02,520 [SGC.py] => Starting model training. Training ALL 2 groups.
2026-01-16 11:43:02,522 [SGC.py] => Unfreezing components for ALL groups: [1, 2]
2026-01-16 11:43:54,207 [SGC.py] => Task: 0, Epoch: 1, Train Loss: 2.076534, Train Acc: 73.7200, Test Acc on New: 93.7000
2026-01-16 11:43:54,225 [SGC.py] => New best accuracy found: 93.70%
